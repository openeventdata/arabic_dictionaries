{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import deque\n",
    "import requests\n",
    "import datetime\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6166"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading actors_id_set : a set that contains the id of all the pages that we have scrapped before\n",
    "#to avoid duplicates. \n",
    "actors_ids_set = pickle.load( open( \"wiki_actors_page_ids.p\", \"rb\" ) )\n",
    "len(actors_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category_members(cmtitle, lang):\n",
    "    global actors_ids_set\n",
    "    wiki_base_url = 'http://en.wikipedia.org/w/api.php' if lang == 'en' else 'http://ar.wikipedia.org/w/api.php'\n",
    "    attributes = {} \n",
    "    attributes['action'] ='query'\n",
    "    attributes['list'] ='categorymembers' #Person name\n",
    "    attributes['cmtitle'] = cmtitle\n",
    "    attributes['cmlimit'] = 500\n",
    "    attributes['cmtype'] = 'page'\n",
    "    attributes['format'] = 'json'\n",
    "    \n",
    "    data = requests.get(wiki_base_url , attributes)\n",
    "    data_json = data.json()\n",
    "    ministers =[]\n",
    "    for i in data_json['query']['categorymembers']:\n",
    "        if i['pageid'] not in actors_ids_set : \n",
    "            actors_ids_set.add(i['pageid'])\n",
    "            if lang == 'en':\n",
    "                ministers.append({\"en_title\": i['title'] , \"en_pageid\" : i['pageid'],\"ar_title\": '' , \"ar_pageid\" : ''   })\n",
    "            else:\n",
    "                ministers.append({\"en_title\": '' , \"en_pageid\" : '',\"ar_title\": i['title'] , \"ar_pageid\" : i['pageid']  })\n",
    "        \n",
    "            \n",
    "    return ministers\n",
    "        \n",
    "\n",
    "def get_lang_links(person_name): \n",
    "    wiki_base_url = 'http://ar.wikipedia.org/w/api.php'\n",
    "    attributes = {} \n",
    "    attributes['action'] ='query'\n",
    "    attributes['titles'] =person_name #Person name\n",
    "    attributes['prop'] = 'langlinks'\n",
    "    attributes['format'] = 'json'\n",
    "    person_lang_links = requests.get(wiki_base_url , attributes)\n",
    "    data = person_lang_links.json()\n",
    "    #check if the person has any other pages with other languages\n",
    " \n",
    "    if list(data['query']['pages'].keys())[0] == '-1' : #person not found\n",
    "        return -1\n",
    "    else : # get arabic  name or any other langue\n",
    "        try:\n",
    "#             print (list(data['query']['pages'].keys())[0])\n",
    "            page_id = list(data['query']['pages'].keys())[0]\n",
    "            links = data['query']['pages'][list(data['query']['pages'].keys())[0]]['langlinks']\n",
    "#             return links\n",
    "            target_link =[]\n",
    "            target_link = [x['*'] for x in links if x['lang'] =='en']\n",
    "            if target_link == []:\n",
    "                return [-2, 0] #unique\n",
    "            \n",
    "            return [target_link[0], page_id]\n",
    "        except KeyError:\n",
    "            return [-2,0]  # don't have english link : unique \n",
    "\n",
    "\n",
    "# test_title = 'تصنيف:وزراء_حكومة_الإمارات_العربية_المتحدة'\n",
    "# x = extract_category_members(test_title , 'ar')\n",
    "\n",
    "# test_title2 = 'Category:Government_ministers_of_the_United_Arab_Emirates'\n",
    "# x2 = extract_category_members(test_title2 , 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Category:Heads_of_government_of_the_Russian_Federation: 100%|██████████| 23/23 [01:42<00:00,  4.72s/it]                    \n"
     ]
    }
   ],
   "source": [
    "#read text file \n",
    "#we could do the processing line by line, but we have a relativly small number of lines \n",
    " #so load them all at onces\n",
    "all_unique_actors = [] # our gold\n",
    "# actors_ids_set = set()\n",
    "wiki_links = []\n",
    "with open('wiki_links3.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    wiki_links = list(reader)\n",
    "wiki_links\n",
    "\n",
    "print(\"Processing .......\")\n",
    "\n",
    "\n",
    "wiki_par = tqdm(wiki_links);\n",
    "\n",
    "for i in wiki_par:\n",
    "    wiki_par.set_description(\"Processing %s\" % i[1].strip())\n",
    "    \n",
    "    if i[0] =='0' : # it is a category for Governments\n",
    "        #Processing English Names\n",
    "        ministers = extract_category_members(i[1] ,'en')\n",
    "        #Processing Arabic \n",
    "        \n",
    "        if(i[2].strip() == 'NAN') : #does not have an equivalent arabic page , just add them\n",
    "            all_unique_actors.extend(ministers)\n",
    "            pass\n",
    "        \n",
    "        else : # check the correspond arabic ones \n",
    "#             print(\"***** : \" , i[1], \"ss\" ,  i[2])\n",
    "            ar_ministers = extract_category_members(i[2] ,'ar')\n",
    "            for j in ar_ministers : \n",
    "                result = get_lang_links(j['ar_title'])\n",
    "                if result[0] == -2 : # unique entry : add \n",
    "                    all_unique_actors.append(j)\n",
    "                else : # it's not unique and we should already have it + append to original data\n",
    "                    target_index = next((index for (index, d) in enumerate(ministers) if d[\"en_title\"] == result[0]), None)\n",
    "                    if target_index == None : # unique, wasn't caught earlier \n",
    "                        if result[0] != -1 : \n",
    "                            all_unique_actors.append({\"en_title\": '' , \"en_pageid\" : '',\"ar_title\": result[0] , \"ar_pageid\" : result[1]  })     \n",
    "                    else : \n",
    "                        ministers[target_index]['ar_title'] = j['ar_title']\n",
    "                        ministers[target_index]['ar_pageid'] = j['ar_pageid']\n",
    "        all_unique_actors.extend(ministers)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unique_actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7316"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actors_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates (if any and remove)\n",
    "#english\n",
    "x = [ x['en_title'] for x in all_unique_actors if x['en_title'] !='']\n",
    "z= set(x)\n",
    "print(len(x) == len(z))\n",
    "#arabic\n",
    "x = [ x['ar_title'] for x in all_unique_actors if x['ar_title'] !='']\n",
    "z= set(x)\n",
    "print(len(x) == len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning duplicate arabic data \n",
    "from collections import Counter\n",
    "dup = [k for k,v in Counter(x).items() if v>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dup : \n",
    "    target_index = next((index for (index, d) in enumerate(all_unique_actors) if d[\"ar_title\"] == dup[0]), None)\n",
    "    del all_unique_actors[target_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing final output and saving progress\n",
    "\n",
    "with open('wiki_actors_3', 'w') as fout:\n",
    "    json.dump(all_unique_actors, fout)\n",
    "    \n",
    "\n",
    "pickle.dump(actors_ids_set , open('wiki_actors_page_ids.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our actors set to be used next time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449\n",
      "2185\n"
     ]
    }
   ],
   "source": [
    "#wiki_links 1 : \n",
    "# 1449\n",
    "# 2185\n",
    "print(len(all_unique_actors))\n",
    "print(len(actors_ids_set))"
   ]
  }
 ],
 "metadata": {
  "gist_id": "7a0473ee97f1f16f180ace4095ab67d1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
