{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import tzutc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actors Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "\n",
    "with open(\"noundict20180225.json\") as f:\n",
    "    for line in f:\n",
    "        nouns.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns[1214]\n",
    "# nouns[12814]['dateStart']['$date']\n",
    "d = parse(nouns[1214]['dateStart']['$date'])\n",
    "print(d)\n",
    "d  > datetime.datetime(1950, 1, 1, 0, 0, tzinfo=tzutc()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_2 = nouns[57:]\n",
    "nouns_by_coders = [i for i in nouns_2 if i['userName'] != 'existEnglishDictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_by_coders[10]\n",
    "nouns[149]\n",
    "type(nouns[149]['dateEnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_to_CAMEO(nouns_by_coders[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_to_CAMEO(one):\n",
    "    if one['dateStart'] == None or one['dateStart']==  'na' :\n",
    "        one['dateStart'] = ''\n",
    "        one['dateEnd'] = ''\n",
    "    \n",
    "    if one['dateStart'] == '' or one['dateEnd'] == '':\n",
    "        print(\"Error : No date Info on Sen {0}\".format(one['sentenceId']))\n",
    "        return\n",
    "    if one['dateEnd'] == 'na' : # throw entry\n",
    "        return \n",
    "    \n",
    "    if one['dateEnd'] == 'now' :\n",
    "#         dateStart = datetime.datetime.strptime(one['dateStart']['$date'].split('T')[0], '%Y-%m-%d')\n",
    "        if (type(one['dateStart']) == str) :\n",
    "            dateStart = parse(one['dateStart'])\n",
    "        else :\n",
    "            dateStart = parse(one['dateStart']['$date'])\n",
    "            \n",
    "        date_range = \" > \" + dateStart.strftime('%Y%m%d')\n",
    "        \n",
    "    else :  \n",
    "\n",
    "#         print(one['dateStart'])\n",
    "        # a if condition else b\n",
    "        dateStart = parse(one['dateStart']) if type(one['dateStart']) == str else parse(one['dateStart']['$date'])\n",
    "        dateEnd  = parse(one['dateEnd']) if type(one['dateEnd']) == str else parse(one['dateEnd']['$date'])\n",
    "\n",
    "\n",
    "        min_date = datetime.datetime(1950, 1, 1, 0, 0, tzinfo=tzutc() )\n",
    "        max_date = datetime.datetime(2050, 1, 1, 0, 0 , tzinfo=tzutc())\n",
    "        \n",
    "\n",
    "        if dateStart < min_date and dateEnd > max_date:\n",
    "            date_range = \"\"\n",
    "        elif dateStart < min_date and dateEnd < min_date:\n",
    "            date_range = \"\"\n",
    "        elif dateStart > min_date and dateEnd < max_date:\n",
    "            date_range = \"{0}-{1}\".format(dateStart.strftime('%Y%m%d'), dateEnd.strftime('%Y%m%d'))\n",
    "        elif dateStart < min_date:\n",
    "            date_range = \"<{0}\".format(dateEnd.strftime('%Y%m%d'))\n",
    "        elif dateEnd > max_date:\n",
    "            date_range = \">{0}\".format(dateStart.strftime('%Y%m%d'))\n",
    "        else:\n",
    "            print(\"whoops \", dateStart, dateEnd)\n",
    "\n",
    "        if date_range:\n",
    "            date_range = \" \" + date_range\n",
    "        \n",
    "        \n",
    "                                   \n",
    "    entry = \"{0} | [{1}{2}{3}{4}]\".format(one['word'], one['countryCode'],\n",
    "                                           one['firstRoleCode'], one['secondRoleCode'],\n",
    "                                           date_range)\n",
    "\n",
    "    return entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entries = [entry_to_CAMEO(i) for i in nouns_by_coders]\n",
    "print(len(entries))\n",
    "entries = [i for i in entries if i]\n",
    "# entries\n",
    "\n",
    "#Removing duplicates while preseving order  ( not the most efficient way)\n",
    "entries_2 = [] \n",
    "[entries_2.append(item) for item in entries if item not in entries_2]\n",
    "entries_2[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_2[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Process Entries to form dictionary \n",
    "from collections import deque \n",
    "master_queue = deque(entries_2) # queue that holds all entires \n",
    "temp_queue = deque() # queue used to hold entries for the same entity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper method for using a queue\n",
    "def pop_q(q):\n",
    "    data = -1\n",
    "    try: \n",
    "        data = q.popleft()\n",
    "        return data\n",
    "    except IndexError: \n",
    "        return -1\n",
    "\n",
    "def peek_q(q): \n",
    "    data = -1\n",
    "    try: \n",
    "        data = q.popleft()\n",
    "        q.appendleft(data)\n",
    "        return data\n",
    "    except IndexError: \n",
    "        return -1\n",
    "\n",
    "\n",
    "processed_entries = [] \n",
    "\n",
    "\n",
    "while True : \n",
    "    entry = pop_q(master_queue)\n",
    "    if entry == -1 : \n",
    "        print(\"Done\")\n",
    "        break\n",
    "    else : \n",
    "        # process (split the entry )\n",
    "        ent_data = entry.split(' | ')\n",
    "        \n",
    "        #Check if it is a single entity (primary phase without dates) example ['مسلحون | [REB]'\n",
    "        #Just add it \n",
    "        if len(ent_data[1].split(' ')) ==1:\n",
    "            processed_entries.append(entry.replace('|' , ''))\n",
    "            \n",
    "        else : \n",
    "            # add the entry to the temp stack that should have similar entires with names\n",
    "            if(len(temp_queue) == 0 ) : # just add it \n",
    "                temp_queue.append(entry)\n",
    "            \n",
    "            else : # if there are elements in the queue, need to check for matching \n",
    "                temp_top = peek_q(temp_queue)\n",
    "                \n",
    "                # check if names are the same : push to queue\n",
    "                temp_ent_data = temp_top.split(' | ')\n",
    "                if (temp_ent_data[0].strip() == ent_data[0].strip() ):\n",
    "                    temp_queue.append(entry)\n",
    "                \n",
    "                #else : we have a punch of entries for the same person/entity with deifferent roles \n",
    "                # process\n",
    "                else : \n",
    "                    master_queue.appendleft(entry)\n",
    "                    \n",
    "                    temp_q_data = list(temp_queue)\n",
    "                    roles =[\"\\t{}\".format(item.split(\" | \")[1]) for item in temp_q_data]\n",
    "#                     if (temp_ent_data[0].strip() == 'نوري المالكي'): \n",
    "#                         print (roles)\n",
    "#                         print(\"**\")\n",
    "#                         print('{}\\n'.format(temp_ent_data[0].strip()) + '\\n'.join(roles))\n",
    "                    processed_entries.append('{}\\n'.format(temp_ent_data[0].strip()) + '\\n'.join(roles))\n",
    "                    temp_queue.clear() \n",
    "                    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arabic_actor_dictionary_v2.txt\", \"w\") as text_file:\n",
    "    text_file.write('\\n'.join(processed_entries))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue = deque([\"Eric\", \"John\", \"Michael\"])\n",
    "# queue.append(\"Terry\")           # Terry arrives\n",
    "# queue.append(\"Graham\")          # Graham arrives\n",
    "# x = queue.popleft()\n",
    "# print(\"popped \", x )\n",
    "# print(queue)\n",
    "# queue.appendleft(x)\n",
    "# print(\"left_appended \", x )\n",
    "# print(queue)\n",
    "# queue.clear()\n",
    "# print(len(queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# entries_2 = [i for i in entries_2 if i]\n",
    "# print(\"**\")\n",
    "# print(len(entries))\n",
    "# print(len(entries_2))\n",
    "\n",
    "# formatted_entries = '\\n'.join(entries)\n",
    "# # formatted_entries =  formatted_entries\n",
    "\n",
    "# formatted_entries[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arabic_actor_dictionary.txt\", \"w\") as text_file:\n",
    "    text_file.write(formatted_entries)\n",
    "    \n",
    "with open(\"entries_2.txt\", \"w\") as text_file: # entreis 2 test\n",
    "    text_file.write('\\n'.join(entries_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [] \n",
    "with open(\"verbs_dict20180227.json\") as vd:\n",
    "    for line in vd : \n",
    "        verbs.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_verbs  = verbs[14:]\n",
    "print('len ', len(arabic_verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_verbs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acceptable_formats = [\"##()()\" , \"##**()\"  , \"##**\" , \"##()\" , \"##()**\"]\n",
    "#core idea is to switch the first and last characters   \n",
    "def verb_not_compound(verb) :\n",
    "    verb_regx = re.findall(\"#(.+)#\" , verb)\n",
    "    if len(verb_regx) == 0 : \n",
    "        return False \n",
    "    if len(verb_regx[0].strip().split(' ')) == 1: \n",
    "        return True\n",
    "    return False \n",
    "\n",
    "    \n",
    "def check_and_reconstruct_verb(verb): \n",
    "    verb = \" \".join(verb.split()) # to remove all white spaces and duplicate ones \n",
    "    if verb == '' :\n",
    "        return 'ERROR'\n",
    "    verb_encod =str(verb.encode('ascii', errors='ignore') , 'UTF8').replace(\" \", \"\")\n",
    "    if verb_encod == '' :  #Lone verb \n",
    "        if verb_not_compound(verb):\n",
    "            return verb\n",
    "        else :\n",
    "            return 'ERROR'\n",
    "#         return ''\n",
    "    elif verb_encod in acceptable_formats: #verb is already correct(rare cases)- apply regex stright\n",
    "        ## Check if one word is between ## : True, return verb, false , Corrupt verb\n",
    "        if verb_not_compound(verb):\n",
    "            return verb\n",
    "        else :\n",
    "            return 'ERROR'\n",
    "    elif verb_encod ==\"##\" : # Lone verbs that could be wrong : do a circular left shift \n",
    "            arr = list(verb)\n",
    "            if arr[0] =='#' : # it is already in the correct format\n",
    "                if verb_not_compound(verb): \n",
    "                    return verb\n",
    "                else :\n",
    "                    return 'ERROR'\n",
    "            \n",
    "            arr= arr[-1:] + arr[:-1]\n",
    "        ## Check if one word is between ## : True, return verb, false , Corrupt verb\n",
    "            c_verb = \"\".join(arr)\n",
    "            if verb_not_compound(c_verb):\n",
    "                return c_verb\n",
    "            else :\n",
    "                return 'ERROR'\n",
    "            \n",
    "    else : \n",
    "    # switch first and last characters \n",
    "        arr = list(verb)\n",
    "        #replace first char with last - op_1\n",
    "        #flip last char \n",
    "        temp = arr[0]\n",
    "        arr[0] = arr[len(arr) -1]\n",
    "        if temp != '*' :\n",
    "            arr[len(arr) -1] = ')'\n",
    "        else :\n",
    "            arr[len(arr) -1] = temp \n",
    "            \n",
    "        #Some verbs are entered inncorretly(coded wrong), at this point we should be able to catch them \n",
    "        reconstructed_string = \"\".join(arr)\n",
    "        rc_string_encod = str(reconstructed_string.encode('ascii', errors='ignore') , 'UTF8').replace(\" \", \"\").strip()\n",
    "        if rc_string_encod in acceptable_formats :\n",
    "        ## Check if one word is between ## : True, return verb, false , Corrupt verb\n",
    "            if verb_not_compound(reconstructed_string):\n",
    "                return reconstructed_string\n",
    "            else : \n",
    "                return 'ERROR'\n",
    "        else :\n",
    "            return 'ERROR'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder to myself:\n",
    "    - '#' goes on either side of the verb (just to indicate what should be '*' in English)\n",
    "    - '*' goes on either side of multi-word direct objects (replaces '{' in English)\n",
    "    - '(' goes around prepositional phrases (same as English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the verbs formatting\n",
    "wrong_verbs =[] \n",
    "correct_verbs = []\n",
    "for verb in arabic_verbs :\n",
    "    rc_string = check_and_reconstruct_verb(verb['word']) # reconstructed string\n",
    "    if rc_string == '' : #lone verbs\n",
    "        print(\"Orignal and correct Verb : \",  verb )\n",
    "        verb['word'] = '#{}#'.format(verb['word'])\n",
    "        print(\"Lone Verb ==> verb \" , verb['word'])\n",
    "        print(\"RegEx\", re.findall(\"#(.+)#\" , verb['word']))\n",
    "        correct_verbs.append(verb)\n",
    "        print(\"\\n************\")\n",
    "    elif rc_string == 'ERROR'  : \n",
    "        print(\"Corrupt Verb (WRONG CODING)\\n\" , verb['word'])\n",
    "        wrong_verbs.append(verb)\n",
    "        print(\"\\n************\")\n",
    "    else :\n",
    "        print(\"Orignal Verb : \",  verb )\n",
    "        print(\"Reconstructed Verb : \" , rc_string)\n",
    "        print(\"RegEx\", re.findall(\"#(.+)#\" , rc_string))\n",
    "        verb['word'] = rc_string\n",
    "        correct_verbs.append(verb)\n",
    "        \n",
    "        print(\"\\n************\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = re.findall('#(.+)#' , '# هدف fdsa#* برجي مركز التجارة العالمي*')\n",
    "# print(x[0].strip().split(' '))\n",
    "# print(len(x[0].strip().split(' ')))\n",
    "# verb_not_compound('#تفجيرانتحاري#')\n",
    "# check_and_reconstruct_verb('#تفجير انتحاري#')\n",
    "# print('Erroneous Verbs Count : ' , str(len(wrong_verbs)))\n",
    "# print('Correct Verbs Count : ' , str(len(correct_verbs)))\n",
    "len(correct_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lemmatize the verbs \n",
    "source_verbs = [] \n",
    "for i in range(len(correct_verbs)) : #Hoping this will preserve order (it is )\n",
    "    \n",
    "    source_verbs.append(re.findall(\"#(.+)#\" , correct_verbs[i]['word'])[0])\n",
    "\n",
    "len(source_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_verbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to copy the output file to Lemmatizer/Input dir \n",
    "with open('verbs_to_lemmatize_v2.txt', 'w') as verbs_file :\n",
    "    verbs_file.write('\\n'.join(source_verbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lemmatizer and get output \n",
    "lem_results = []\n",
    "#had to manually remove last line (empty) from file \n",
    "with open('verbs_to_lemmatize.txt.ATB4MT.tok' , 'r') as file : \n",
    "    lem_results = file.read()\n",
    "\n",
    "lem_results = lem_results.split('\\n')\n",
    "len(lem_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(source_verbs))\n",
    "print(len(lem_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(source_verbs) == len(lem_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ='ب+·ب+·بِ+·PRT+·IN+·PREP+ قصف·قصف·قَصْف·NOM·NN·NOUN+CASE_DEF_GEN'\n",
    "y = x.split(' ')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_verbs = []\n",
    "for i in lem_results : \n",
    "#     print(i)\n",
    "    res = i.split(' ')\n",
    "    print(res)\n",
    "    correct_one =''\n",
    "    for i in res : \n",
    "#         if 'NOM' in i : \n",
    "#             correct_one = i \n",
    "#             break\n",
    "#         if 'VRB' in i :\n",
    "#             correct_one = i\n",
    "#             break \n",
    "        if 'PROP' in i  or 'PRT' in i or 'NOM' in i or 'VRB' in i : \n",
    "            correct_one = i \n",
    "    if correct_one == '' : \n",
    "        print('wtf ' , i )\n",
    "        continue\n",
    "    lem_verbs.append([correct_one.split('·')[2] ,correct_one.split('·')[0]]) #lemma , source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_verbs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything back together \n",
    "for i in range(len(correct_verbs)) : #Trust its the right order ....\n",
    "    correct_verbs[i]['verb_lemma'] = lem_verbs[i][0]\n",
    "# correct_verbs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('verbs_with_lemmas.json', 'w') as output : \n",
    "    for i in correct_verbs : \n",
    "        output.write('{}\\n'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_verbs_3.txt', 'w') as output : \n",
    "    for i in lem_verbs : \n",
    "        output.write('{}\\t{}\\t{}\\n'.format(i[0], '-' , 'متعدي'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the conjugations and put ALL TOGETHER AND DONE \n",
    "conj =''\n",
    "with open('protest_verbs.txt' , 'r') as input_f:\n",
    "    conj= input_f.read()\n",
    "\n",
    "# conj = conj.encode('UTF-8')\n",
    "conj = conj.split('\\n')\n",
    "conj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for i in range(len(conj)) : #Trust its the right order ....\n",
    "    conj_list = eval(eval(conj[i])['conjugations'])\n",
    "    if conj_list[0] == 'invalid':\n",
    "        continue\n",
    "    \n",
    "#     print(conj_list)\n",
    "    total.extend(conj_list)\n",
    "\n",
    "# total   \n",
    "with open('protest_conjugated.txt', 'w') as output : \n",
    "    output.write('\\n'.join(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in correct_verbs : \n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point\n",
    "with open('complete_verb_date.json', 'w') as output : \n",
    "    for i in correct_verbs : \n",
    "        output.write('{}\\n'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the check point\n",
    "loaded_verbs = []\n",
    "with open('complete_verb_date.json', 'r') as input_file : \n",
    "    for line in input_file : \n",
    "        loaded_verbs.append(eval(line))\n",
    "\n",
    "loaded_verbs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the 'dic' \n",
    "import re\n",
    "def create_verb_bock(verb) : \n",
    "    #check if verb is lone verb (no patters)-- then 'verbcode' is it's main code, else main code is null\n",
    "\n",
    "    if verb['verb_conj'][0] == 'invalid' :\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    entry = ''\n",
    "    lone_verb = False \n",
    "    if len(verb['word'].split(' ')) == 1 : \n",
    "        lone_verb = True\n",
    "#     print(verb['verbcode'])\n",
    "    if lone_verb : \n",
    "#         entry = '--- {}  [{}] ---\\n'.format(re.findall(\"#(.+)#\" , verb['word'])[0] , verb['verbcode'])\n",
    "        entry = '--- {}  [{}] ---\\n'.format(verb['main_verb'], verb['verbcode'])\n",
    "        entry += '{{ {} }}\\n\\n'.format(' '.join(verb['verb_conj']))\n",
    "    \n",
    "    else  : # not lone verb\n",
    "#         entry = '--- {}  [---] ---\\n'.format(re.findall(\"#(.+)#\" , verb['word'])[0])\n",
    "        entry = '--- {}  [---] ---\\n'.format(verb['main_verb'])\n",
    "#         entry += '{{ {} }} \\n\\n'.format(' '.join(verb['verb_conj']))\n",
    "        entry += '- {}\\t\\t[{}]\\n\\n'.format(re.sub(r'#(.+)#' , '* ' , verb['word']), verb['verbcode'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return entry\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_verb_bock(loaded_verbs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = ''\n",
    "for i in loaded_verbs : \n",
    "#     print(i)\n",
    "    final_output += create_verb_bock(i) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VERBS.DICT.AR.LEMMA.V3.txt' , 'w') as output: \n",
    "    output.write(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{{{}}}'.format('khaled')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
